{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "tcn8rUQyXmiN"
   },
   "outputs": [],
   "source": [
    "# !pip install langchain==0.1.19\n",
    "# !pip install langchain-openai==0.1.6\n",
    "# !pip install langchain-community==0.0.38\n",
    "# %pip install -qU pypdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "9SEZYE0wX8Hs"
   },
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "from getpass import getpass\n",
    "import os\n",
    "\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "from langchain_core.prompts import ChatPromptTemplate,PromptTemplate\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.output_parsers import PydanticOutputParser\n",
    "\n",
    "from pydantic import BaseModel, Field\n",
    "from typing import Optional\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "BjHVY8RS-fZ9",
    "outputId": "ae309a1c-0ad8-4544-a2e6-53b7f95f668d"
   },
   "outputs": [],
   "source": [
    "file_path = r\"...path_to_pdf\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "HDowR-24X-kI",
    "outputId": "be7b06dd-024c-4468-dcf2-367d32a087fd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "··········\n"
     ]
    }
   ],
   "source": [
    "OPENAPI_KEY = getpass()\n",
    "os.environ['OPENAI_API_KEY'] = OPENAPI_KEY"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xMav0drvYdBz"
   },
   "source": [
    "# Study Assistant\n",
    "\n",
    "## 1. Read the uploaded study material (only pdf)\n",
    "## 2. Generate Summary\n",
    "## 3. Create Quiz Questions with answer options"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qxw7Cf5GAGOX"
   },
   "source": [
    "## Load input file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "v3EIwiJSchX-"
   },
   "outputs": [],
   "source": [
    "loader = PyPDFLoader(file_path)\n",
    "input_file = loader.lazy_load()\n",
    "content = \" \".join([page.page_content for page in input_file])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5BOXIDEIANoa"
   },
   "source": [
    "## Output Format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "xmtxnnToncpd",
    "outputId": "bf4f868e-ef64-476d-edc6-b43c9cb04d1c"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PydanticOutputParser(pydantic_object=<class '__main__.MCQ'>)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class MainPoints(BaseModel):\n",
    "  point_number: int = Field(description=\"Point Number\")\n",
    "  point_text: str = Field(description=\"Main Points of the summary\")\n",
    "\n",
    "class Option(BaseModel):\n",
    "    option_id: str = Field(description=\"A,B,C,D\")\n",
    "    answer_text: str = Field(description=\"option text\")\n",
    "\n",
    "class Question(BaseModel):\n",
    "    question_no: int = Field(description=\"Question Number\")\n",
    "    question: str = Field(description=\"Quiz Question\")\n",
    "    options: list[Option]\n",
    "    difficulty: str = Field(description = \"The difficulty level of the question. Easy, Medium or Hard\")\n",
    "    answer : str = Field(description=\"Correct Answer\")\n",
    "\n",
    "\n",
    "class MCQ(BaseModel):\n",
    "    topic: str = Field(description=\"Topic of the study material\")\n",
    "    summary: str = Field(description=\"Summary of the study material\")\n",
    "    main_points: list[MainPoints]\n",
    "    questions: list[Question]\n",
    "\n",
    "parser = PydanticOutputParser(pydantic_object=MCQ)\n",
    "parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "uB3uyrIAYyyo"
   },
   "outputs": [],
   "source": [
    "prompt_text = \"\"\"You are a brilliant study assistant, you read through the content provided by the user and summarize it.\n",
    "The summary should contain\n",
    "- numbered bullet points for each main point with a crisp and concise explanation.\n",
    "\n",
    "Once you are done with the summarizing, generate a quiz with {no_of_questions} number of multiple choice questions from the summarized content ,\n",
    "each with its own set of answer options. The questions should\n",
    "- be from the study content only\n",
    "- range from easy, medium to hard levels\n",
    "- test the user's knowledge of the content\n",
    "- should not be the same option number for all the questions\n",
    "Format Instructions:{format_instructions}\n",
    "content : {content}\n",
    "no_of_questions : {no_of_questions} \"\"\"\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "    template=prompt_text,\n",
    "    input_variables=[\"content\",\"no_of_questions\"],\n",
    "    partial_variables={\"format_instructions\": parser.get_format_instructions()},\n",
    ")\n",
    "\n",
    "#prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "a0ZDCbp1feHu"
   },
   "outputs": [],
   "source": [
    "model = ChatOpenAI(model_name=\"gpt-3.5-turbo\", temperature=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "nnxJ2bZBo89K"
   },
   "outputs": [],
   "source": [
    "chain = ( prompt | model | parser)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "BvTV13aapAsV"
   },
   "outputs": [],
   "source": [
    "response = chain.invoke({\"content\": content,\"no_of_questions\":5})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "bM4r6Z5XpfXv",
    "outputId": "0488e0ec-cc04-42ee-f7f2-34117d0f67fd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic : Prompt Engineering in AI\n",
      "\n",
      "Summary : Prompt engineering is a practice within natural language processing (NLP) in artificial intelligence, where text is used to describe the task the AI should perform. Prompts are detailed descriptions of the desired output from an AI model. Examples of prompts include text prompts, code prompts, and image prompts. To engineer effective AI prompts, one should consider role-playing, clarity, specification, and consistency. Standard prompt patterns include user-model interaction, few-shot prompting, and question-and-answer pattern. Advanced prompting techniques include zero-shot prompting, few-shot prompting, and chain-of-thought (CoT). Pitfalls to avoid when creating prompts include information overload, open-ended questions, and poor use of constraints.\n",
      "\n",
      "Highlights : \n",
      "\n",
      "1. Prompt engineering involves using text to describe the task an AI should perform.\n",
      "2. Prompts are detailed descriptions of the desired output from an AI model.\n",
      "3. To engineer effective AI prompts, consider role-playing, clarity, specification, and consistency.\n",
      "4. Standard prompt patterns include user-model interaction, few-shot prompting, and question-and-answer pattern.\n",
      "5. Advanced prompting techniques include zero-shot prompting, few-shot prompting, and chain-of-thought (CoT).\n",
      "\n",
      "1. What is the goal of prompt engineering in AI? \n",
      "\n",
      "Difficulty level : easy \n",
      "\n",
      "A. To describe the task an AI should perform.\n",
      "B. To confuse the AI model.\n",
      "C. To generate random outputs.\n",
      "D. To limit the AI's capabilities.\n",
      "\n",
      " Answer : A\n",
      "\n",
      "\n",
      "2. What are prompts in the context of AI? \n",
      "\n",
      "Difficulty level : easy \n",
      "\n",
      "A. Random inputs to the AI model.\n",
      "B. Detailed descriptions of desired AI model outputs.\n",
      "C. Irrelevant information for the AI model.\n",
      "D. Limitations imposed on the AI model.\n",
      "\n",
      " Answer : B\n",
      "\n",
      "\n",
      "3. What should be considered to engineer effective AI prompts? \n",
      "\n",
      "Difficulty level : medium \n",
      "\n",
      "A. Complexity and ambiguity.\n",
      "B. Role-playing, clarity, specification, and consistency.\n",
      "C. Randomness and inconsistency.\n",
      "D. Lack of detail and specificity.\n",
      "\n",
      " Answer : B\n",
      "\n",
      "\n",
      "4. Which of the following is a standard prompt pattern? \n",
      "\n",
      "Difficulty level : medium \n",
      "\n",
      "A. User-Model Interaction.\n",
      "B. Random Output Generation.\n",
      "C. Unspecified Task Description.\n",
      "D. Model-User Interaction.\n",
      "\n",
      " Answer : A\n",
      "\n",
      "\n",
      "5. What is an advanced prompting technique in AI? \n",
      "\n",
      "Difficulty level : hard \n",
      "\n",
      "A. Random-Shot Prompting.\n",
      "B. Few-Shot Prompting.\n",
      "C. Unspecified Prompting.\n",
      "D. Model-User Prompting.\n",
      "\n",
      " Answer : B\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(f\"Topic : {response.topic}\\n\")\n",
    "print(f\"Summary : {response.summary}\\n\")\n",
    "print(f\"Highlights : \\n\")\n",
    "for point in response.main_points:\n",
    "  print(f\"{point.point_number}. {point.point_text}\")\n",
    "for question in response.questions:\n",
    "  print(f\"\\n{question.question_no}. {question.question} \\n\")\n",
    "  print(f\"Difficulty level : {question.difficulty} \\n\")\n",
    "  for option in question.options:\n",
    "    print(f\"{option.option_id}. {option.answer_text}\")\n",
    "\n",
    "  print(f\"\\n Answer : {question.answer}\\n\")"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
